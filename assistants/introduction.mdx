---
title: Introduction
---

**Assistants** add memory, knowledge and tools to LLMs.

We **"prompt"** them using `description` and `instructions`, for example:

```python assistant.py
from phi.assistant import Assistant
from phi.llm.openai import OpenAIChat

assistant = Assistant(
    llm=OpenAIChat(model="gpt-3.5-turbo"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
    debug_mode=True,
)

# -*- Print the response in markdown format
assistant.print_response("Share a breakfast recipe.", markdown=True)
```

Under the hood, the description and instructions are converted into the system prompt and the input is passed as the user prompt. Use `debug_mode=True` to print the underlying prompts sent to the LLM, here's how they look like:

```js
DEBUG    ============== system ==============
DEBUG    You help people with their health and fitness goals.
         YOU MUST FOLLOW THESE INSTRUCTIONS CAREFULLY.
         <instructions>
         1. Recipes should be under 5 ingredients.
         2. Use markdown to format your answers.
         </instructions>
DEBUG    ============== user ==============
DEBUG    Share a breakfast recipe.
DEBUG    Time to generate response: 1.8701s
DEBUG    Estimated completion tokens: 113
```

## Return response as a variable

Use the `Assistant.run()` function to return the LLM response as a variable.

```python
# -*- Get the response as a stream
response = ""
for delta in assistant.run('Share a breakfast recipe.'):
    response += delta
```

By default `stream=True`, set `stream=False` to return as a string.

```python
# -*- Get the response as a string
response = assistant.run('Share a breakfast recipe.', stream=False)
```

## Run the Assistant as a CLI app

Run the Assistant as a command line application using `assistant.cli_app()`

```python
from phi.assistant import Assistant
from phi.tools.duckduckgo import DuckDuckGo

assistant = Assistant(tools=[DuckDuckGo()], show_tool_calls=True, read_chat_history=True)
assistant.cli_app(markdown=True)
```

Ask

```
Whats happening in france?
```

Then ask:

```
Summarize the last response
```

## Manually set the system prompt

You can manually set the system prompt using the `system_prompt` variable. The `description` and `instructions` only provide a formatting benefit. Learn more in the [instructions](/assistants/instructions) guide.

```python
from phi.assistant import Assistant

assistant = Assistant(system_prompt="Share a 2 sentence story about")
assistant.print_response("Love in the year 12000.")
```
