---
title: Build a Multimodal LLM App
---

## What is Multimodal?

Multimodal means the LLM can work with images along with text. Historically, language model systems have been limited to text but **GPT-4V** can also understand images (and with some workarounds - videos too). This unlocks a powerful set of new use-cases for LLMs.

Let's build a Multimodal LLM App powered by `GPT-4V`. We'll serve the app using Streamlit and FastApi and use Postgres for Storage.

By the end of this guide you'll be ready to build your own Multimodal LLM product.

<Note>

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) to run this app locally.

</Note>

## Setup

Open the `Terminal` and create an `ai` directory with a python virtual environment.

<CodeGroup>

```bash Mac
mkdir ai && cd ai

python3 -m venv aienv
source aienv/bin/activate
```

```bash Windows
mkdir ai; cd ai

python3 -m venv aienv
aienv/scripts/activate
```

</CodeGroup>

Install `phidata`

<CodeGroup>

```bash Mac
pip install -U phidata
```

```bash Windows
pip install -U phidata
```

</CodeGroup>

<Note>

If you encounter errors, update pip using `python -m pip install --upgrade pip`

</Note>

## Create your codebase

Create your codebase using the `llm-app` template pre-configured with [FastApi](https://fastapi.tiangolo.com/), [Streamlit](https://streamlit.io/) and [PgVector](https://github.com/pgvector/pgvector). We'll use this codebase as the starting point for our RAG LLM App.

<CodeGroup>

```bash Mac
phi ws create -t llm-app -n llm-app
```

```bash Windows
phi ws create -t llm-app -n llm-app
```

</CodeGroup>

<Note>

If you already have an `llm-app` created, use that instead.

</Note>

<ImageContainer src="/gifs/create_llm_app.gif" alt="Creat LLM App" />

This will create a folder named `llm-app` with the following structure:

```bash llm-app
llm-app                     # root directory for your Autonomous llm-app
├── api                     # directory for FastApi routes
├── app                     # directory for Streamlit apps
├── db                      # directory for database components
├── llm                     # directory for LLM components
    ├── conversations       # LLM conversations
    ├── knowledge_base.py   # LLM knowledge base
    └── storage.py          # LLM storage
├── notebooks               # directory for Jupyter notebooks
├── Dockerfile              # Dockerfile for the application
├── pyproject.toml          # python project definition
├── requirements.txt        # python dependencies managed by pyproject.toml
├── scripts                 # directory for helper scripts
├── tests                   # directory for unit tests
├── utils                   # directory for shared utilities
└── workspace               # phidata workspace directory
    ├── dev_resources.py    # dev resources running locally
    ├── prd_resources.py    # production resources running on AWS
    ├── jupyter             # jupyter notebook resources
    ├── secrets             # directory for storing secrets
    └── settings.py         # phidata workspace settings
```

### Optional: Set OpenAI Api Key

To use your own `OPENAI_API_KEY`, set the `OPENAI_API_KEY` environment variable. You can get one [from OpenAI here](https://platform.openai.com/account/api-keys). Otherwise `phi` provides ready to use access to OpenAI models.

<CodeGroup>

```bash Mac
export OPENAI_API_KEY=sk-***
```

```bash Windows
setx OPENAI_API_KEY sk-***
```

</CodeGroup>

## Serve your App using Streamlit

[Streamlit](https://streamlit.io/) allows us to build
quick micro front-ends which we will use to test our multimodal app. Start the `app`
group using:

<CodeGroup>

```bash terminal
phi ws up --group app
```

```bash shorthand
phi ws up dev:docker:app
```

</CodeGroup>

<Note>

You might need to pull the latest image using `phi ws up --group app -f`

</Note>

<ImageContainer src="/gifs/phi_ws_up_app.gif" alt="Run LLM App" />

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### Chat with Images

- Open [localhost:8501](http://localhost:8501) to view streamlit apps that you can customize and make your own.
- Click on **Chat with Images** in the sidebar
- Enter a username.
- Upload an image and ask questions
- Click on `Generate Caption`, `Describe Image`, `Identify Brands`, `Identify Items` for examples.

<ImageContainer
  src="/images/chat-with-images-local.png"
  alt="chat-with-images-local"
/>

## How this App works

This streamlit app is defined in the `app/pages/2_Chat_with_Images.py` file and the `Conversation` powering these apps are defined in the `llm/conversations/vision.py` file.

## Serve your App using FastApi

Streamlit is great for building micro front-ends but any production application will be built using a front-end framework like [next.js](https://nextjs.org) backed by a RestApi built using [FastApi](https://fastapi.tiangolo.com/).

Your LLM App comes pre-configured with FastApi, start the `api` group using:

<CodeGroup>

```bash terminal
phi ws up --group api
```

```bash shorthand
phi ws up dev:docker:api
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download.

- Open [localhost:8000/docs](http://localhost:8000/docs) to view the API Endpoints.

<Warning>

We are working on building a set of common endpoints for this multimodal LLM App.

</Warning>

## Optional: Run Jupyterlab

A jupyter notebook is a must have for AI development and your `llm-app` comes with a notebook pre-installed with the required dependencies. Enable it by updating the `workspace/settings.py` file:

```python workspace/settings.py
...
ws_settings = WorkspaceSettings(
    ...
    # Uncomment the following line
    dev_jupyter_enabled=True,
...
```

Start `jupyter` using:

<CodeGroup>

```bash terminal
phi ws up --group jupyter
```

```bash shorthand
phi ws up dev:docker:jupyter
```

</CodeGroup>

<Note>

You might need to pull the latest image using `phi ws up --group jupyter -f`

</Note>

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### View Jupyterlab UI

- Open [localhost:8888](http://localhost:8888) to view the Jupyterlab UI. Password: **admin**
- Open `notebooks/vision` to play around with `GPT-4V`

<ImageContainer
  src="/images/multimodal-llm-app-jupyter-local.png"
  alt="multimodal-llm-app-jupyter-local"
/>

## Delete local resources

Play around and stop the workspace using:

<CodeGroup>

```bash terminal
phi ws down
```

```bash full options
phi ws down --env dev --infra docker
```

```bash shorthand
phi ws down dev:docker
```

</CodeGroup>

or stop individual Apps using:

<CodeGroup>

```bash App
phi ws down --group app
```

```bash Api
phi ws down --group api
```

```bash PgVector
phi ws down --group db
```

```bash Jupyter
phi ws down --group jupyter
```

</CodeGroup>

---

## Run on AWS

Now let's run the **LLM App** in production on AWS.

## AWS Authentication

To run on AWS, you need **one** of the following:

1. The `~/.aws/credentials` file with your AWS credentials
2. `AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` environment variables

<Note>

To create the credentials file, install the [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and run `aws configure`

</Note>

### Add AWS Region and Subnets

Add 2 [Subnets](https://us-east-1.console.aws.amazon.com/vpc/home?#subnets:) to the `workspace/settings.py` file, these are required for the ECS services.

```python workspace/settings.py
ws_settings = WorkspaceSettings(
    ...
    # -*- AWS settings
    # Add your Subnet IDs here
    subnet_ids=["subnet-xyz", "subnet-xyz"],
    ...
)
```

<Note>

Please check that the subnets belong to the selected `aws_region`

</Note>

## Update Secrets

Update the streamlit app password in the `workspace/secrets/prd_app_secrets.yml` file:

```python workspace/secrets/prd_app_secrets.yml
APP_PASSWORD: "admin"
# OPENAI_API_KEY: "sk-***"
```

Update the RDS database password in the `workspace/secrets/prd_db_secrets.yml` file:

```python workspace/secrets/prd_db_secrets.yml
# Secrets used by prd RDS database
MASTER_USERNAME: llm
MASTER_USER_PASSWORD: "llm9999!!"
```

## Create AWS resources

Create AWS resources for the LLM App using:

<CodeGroup>

```bash terminal
phi ws up --env prd --infra aws
```

```bash shorthand
phi ws up prd:aws
```

</CodeGroup>

This will create:

1. [ECS Cluster](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusters.html) for the application.
2. [ECS Task Definitions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) and [Services](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html) that run the application on the ECS cluster.
3. [LoadBalancer](https://aws.amazon.com/elasticloadbalancing/) to route traffic to the application.
4. [Security Groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) that control incoming and outgoing traffic.
5. [Secrets](https://aws.amazon.com/secrets-manager/) for managing application and database secrets.
6. [RDS Database](https://aws.amazon.com/rds/) for Knowledge Base and Storage.

**Press Enter** to confirm and grab a cup of coffee while the resources spin up.

- The RDS database takes about 10 minutes to activate.
- These resources are defined in the `workspace/prd_resources.py` file.
- Use the [ECS console](https://us-east-1.console.aws.amazon.com/ecs/v2/clusters) to view services and logs.
- Use the [RDS console](https://us-east-1.console.aws.amazon.com/rds/home?#databases:) to view the database instance.

<ImageContainer
  src="/gifs/create_llm_app_aws.gif"
  alt="create_llm_app_aws.gif"
/>

## Production Streamlit App

**Open the LoadBalancer DNS** provided when creating the Streamlit App

### Chat with Images

- Enter the `APP_PASSWORD` from the `prd_app_secrets.yml` file (default: `admin`)
- Click on **Chat with Images** in the sidebar
- Enter a username.
- Upload an image and ask questions
- Click on `Generate Caption`, `Describe Image`, `Identify Brands`, `Identify Items` for examples.

<ImageContainer
  src="/images/chat-with-images-local.png"
  alt="chat-with-images-local"
/>

## Production FastApi

- **Open the LoadBalancer DNS** + the `/docs` endpoint to view the API Endpoints.

<Warning>

We are working on building a set of common endpoints for this multimodal LLM App.

</Warning>

## Update Production

To update the production application, follow [this guide](/how-to/production-app) to

1. Create a new image
2. Update the ECS Task Definition and Services.

## Delete AWS resources

Play around and then delete AWS resources using:

<CodeGroup>

```bash terminal
phi ws down --env prd --infra aws
```

```bash shorthand
phi ws down prd:aws
```

</CodeGroup>

or delete individual resource groups using:

<CodeGroup>

```bash App
phi ws down --env prd --infra aws --group app
```

```bash Database
phi ws down --env prd --infra aws --group db
```

</CodeGroup>

## Next

Congratulations on building a production-ready RAG LLM App with access to a knowledge base and storage. Next Steps:

- Modify and make this App your own.
- Create a [git repository for this workspace](/how-to/git-repo).
- Read how to [update the dev application](/how-to/dev-app).
- Read how to [update the production application](/how-to/production-app).
- Read how to [format and validate your code](/how-to/format-and-validate).
- Read how to [add python libraries](/how-to/python-libraries).
- Read how to [add database tables](/how-to/database-tables)
- Read how to [add a custom domain and HTTPS](/how-to/domain-https).
- Read how to [implement CI/CD](/how-to/ci-cd)
- Learn about other [day-2 operations](/how-to/how-to-operations).
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8).
