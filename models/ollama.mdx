---
title: Ollama
---

Run Large Language Models locally with Ollama

[Ollama](https://ollama.com) is a fantastic tool for running LLMs locally. Install [ollama](https://ollama.com) and run a model using

<CodeGroup>

```bash run model
ollama run llama2
```

```bash serve
ollama serve
```

</CodeGroup>

After you have the local model running, use the `Ollama` LLM to access them

## Example

<CodeGroup>

```python agent.py
from phi.agent import Agent
from phi.llm.ollama import Ollama

agent = Agent(
    llm=Ollama(model="llama3"),
    description="You help people with their health and fitness goals.",
)
agent.print_response("Share a quick healthy breakfast recipe.", markdown=True)
```

</CodeGroup>

## Params

<ResponseField name="name" type="str" default="Ollama">
  The name of the LLM.
</ResponseField>
<ResponseField name="model" type="str" default="openhermes">
  The name of the model to be used.
</ResponseField>
<ResponseField name="host" type="str">
  The host URL for making API requests to the service.
</ResponseField>
<ResponseField name="format" type="str">
  The response format, either an empty string for default or "json" for JSON responses.
</ResponseField>
<ResponseField name="timeout" type="Any">
  The timeout duration for requests, can be specified in seconds.
</ResponseField>
<ResponseField name="options" type="Dict[str, Any]">
  A dictionary of options to include with the request, e.g., `{"temperature": 0.1, "stop": ["\n"]}`.
</ResponseField>
<ResponseField name="keep_alive" type="Union[float, str]">
  The keep-alive duration for maintaining persistent connections, can be specified in seconds or as a string.
</ResponseField>
<ResponseField name="client_kwargs" type="Dict[str, Any]">
  Additional keyword arguments provided as a dictionary when initializing the `Ollama()` client.
</ResponseField>
<ResponseField name="ollama_client" type="ollama.Client">
  An instance of `ollama.Client` provided for making API requests.
</ResponseField>
